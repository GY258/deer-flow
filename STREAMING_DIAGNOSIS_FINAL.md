# LangGraph æµå¼è¾“å‡ºé—®é¢˜ - å®Œæ•´æ’æŸ¥æŠ¥å‘Š

## âœ… æ’æŸ¥æ€»ç»“

### ç¬¬ 0 æ­¥ï¼šåŸºçº¿å¯¹ç…§ âœ…

**æµ‹è¯•è„šæœ¬**ï¼š`test_openai_stream_baseline.py`

**ç»“æœ**ï¼š
```
âœ… æ¨¡å‹ç«¯ç¡®å®åœ¨é€ token æ¨é€ï¼ˆæµå¼è¾“å‡ºæ­£å¸¸ï¼‰
   æ€» token æ•°: 29
   é¦– token å»¶è¿Ÿ (TTFB): 0.01s
   æµå¼ä¼ è¾“è€—æ—¶: 0.31s
   â†’ å¦‚æœ LangGraph ä¸­ä¸æ˜¯æµå¼çš„ï¼Œé—®é¢˜åœ¨ LangGraph/æœåŠ¡ç«¯
```

**ç»“è®º**ï¼šOpenAI API å’Œ SDK å·¥ä½œæ­£å¸¸ï¼Œé—®é¢˜åœ¨ LangGraph/æœåŠ¡ç«¯å®ç°ã€‚

---

### ç¬¬ 1 æ­¥ï¼šLangGraph å±‚æ’æŸ¥ âœ…

**æ£€æŸ¥ä½ç½®**ï¼š`src/server/app.py`

#### æ£€æŸ¥é¡¹ 1ï¼šæœåŠ¡ç«¯æ˜¯å¦å…ˆæ”¶é›†å†è¿”å›ï¼Ÿ

```python
# src/server/app.py ç¬¬ 401-418 è¡Œ
async for event in _stream_graph_events(...):
    if request_logger:
        _log_event_data(...)
    yield event  # âœ… é€ä¸ª yieldï¼Œæ­£ç¡®ï¼
```

**ç»“è®º**ï¼šâœ… æœåŠ¡ç«¯æ²¡æœ‰å…ˆæ”¶é›†å†è¿”å›çš„é—®é¢˜ã€‚

#### æ£€æŸ¥é¡¹ 2ï¼šä½¿ç”¨äº†æ­£ç¡®çš„æµå¼æ–¹æ³•ï¼Ÿ

```python
# src/server/app.py ç¬¬ 276-280 è¡Œ
async for agent, _, event_data in graph_instance.astream(  # âœ… ä½¿ç”¨ astream
    workflow_input,
    config=workflow_config,
    stream_mode=["messages", "updates"],  # âœ… æ­£ç¡®çš„ stream_mode
    subgraphs=True,
):
```

**ç»“è®º**ï¼šâœ… ä½¿ç”¨äº†æ­£ç¡®çš„ `astream` å’Œ `stream_mode`ã€‚

---

### ç¬¬ 2 æ­¥ï¼šèŠ‚ç‚¹å±‚æ’æŸ¥ âŒ

**æ£€æŸ¥ä½ç½®**ï¼š`src/graph/nodes.py`

#### é—®é¢˜ä»£ç  1ï¼š`reporter_node`ï¼ˆç¬¬ 336-359 è¡Œï¼‰

```python
def reporter_node(state: State, config: RunnableConfig):
    llm = get_llm_by_type(AGENT_LLM_MAP["reporter"])
    
    response_content = ""
    for chunk in llm.stream(invoke_messages):  # âŒ èŠ‚ç‚¹æ¶ˆè´¹äº†æµ
        if getattr(chunk, "content", None):
            response_content += chunk.content
    
    return {
        "final_report": response_content,
        "messages": [AIMessage(content=response_content, name="reporter")],  # âŒ ä¸€æ¬¡æ€§è¿”å›
    }
```

#### é—®é¢˜ä»£ç  2ï¼š`simple_researcher_node`ï¼ˆç¬¬ 870-908 è¡Œï¼‰

```python
async def simple_researcher_node(state: State, config: RunnableConfig):
    llm = get_llm_by_type(AGENT_LLM_MAP["reporter"])
    
    response_content = ""
    try:
        for chunk in llm.stream(invoke_messages):  # âŒ èŠ‚ç‚¹æ¶ˆè´¹äº†æµ
            if getattr(chunk, "content", None):
                response_content += chunk.content
    except Exception as e:
        # ...
    
    return Command(
        update={
            "final_report": response_content,  # âŒ ä¸€æ¬¡æ€§è¿”å›
        }
    )
```

**é—®é¢˜åˆ†æ**ï¼š
1. èŠ‚ç‚¹å†…éƒ¨è°ƒç”¨ `llm.stream()` å¹¶ç”¨ for å¾ªç¯æ¶ˆè´¹äº†æ‰€æœ‰ chunk
2. LangGraph çš„ `stream_mode="messages"` æ— æ³•æ•è·å·²è¢«æ¶ˆè´¹çš„ chunk
3. èŠ‚ç‚¹æœ€åä¸€æ¬¡æ€§è¿”å›å®Œæ•´çš„ `AIMessage`
4. ç»“æœï¼šå‰ç«¯åªèƒ½æ”¶åˆ°ä¸€æ¬¡æ€§è¿”å›çš„å®Œæ•´å†…å®¹

**è¿™å°±æ˜¯é—®é¢˜çš„æ ¹æœ¬åŸå› ï¼**

---

## ğŸ’¡ ä¿®å¤æ–¹æ¡ˆ

### æ–¹æ¡ˆ Aï¼šæ”¹ç”¨ invoke + é…ç½® streaming=Trueï¼ˆæ¨èï¼‰

#### æ­¥éª¤ 1ï¼šä¿®æ”¹ LLM é…ç½®

```python
# æ–‡ä»¶ï¼šsrc/llms/llm.py ç¬¬ 101 è¡Œ
def _create_llm_use_conf(llm_type: LLMType, conf: Dict[str, Any]) -> BaseChatModel:
    # ... å‰é¢ä»£ç ä¸å˜ ...
    
    if llm_type == "reasoning":
        merged_conf["api_base"] = merged_conf.pop("base_url", None)
        return ChatDeepSeek(**merged_conf)
    else:
        # âœ… æ·»åŠ  streaming=True
        return ChatOpenAI(**merged_conf, streaming=True)
```

#### æ­¥éª¤ 2ï¼šä¿®æ”¹ reporter_node

```python
# æ–‡ä»¶ï¼šsrc/graph/nodes.py ç¬¬ 336-359 è¡Œ
def reporter_node(state: State, config: RunnableConfig):
    """Reporter node that write a final report."""
    logger.info("Reporter write final report")
    configurable = Configuration.from_runnable_config(config)
    # ... å‰é¢çš„å‡†å¤‡å·¥ä½œä¸å˜ ...
    
    llm = get_llm_by_type(AGENT_LLM_MAP["reporter"])
    
    try:
        logger.info("Reporterå¼€å§‹ç”ŸæˆæŠ¥å‘Š...")
        # âœ… æ”¹ç”¨ invokeï¼Œä¸æ¶ˆè´¹æµ
        response = llm.invoke(invoke_messages)
        response_content = response.content if hasattr(response, 'content') else str(response)
        
        if not response_content:
            logger.warning("Reporterå†…å®¹ä¸ºç©º")
            response_content = "æŠ±æ­‰ï¼Œç”ŸæˆæŠ¥å‘Šå¤±è´¥ã€‚"
            
    except Exception as e:
        logger.error(f"Reporter LLMè°ƒç”¨å¼‚å¸¸: {e}", exc_info=True)
        response_content = f"æŠ±æ­‰ï¼Œç”ŸæˆæŠ¥å‘Šæ—¶å‡ºç°é”™è¯¯: {str(e)}"
    
    logger.info(f"reporter response length: {len(response_content)}")
    
    return {
        "final_report": response_content,
        "messages": [AIMessage(content=response_content, name="reporter")],
    }
```

#### æ­¥éª¤ 3ï¼šä¿®æ”¹ simple_researcher_node

```python
# æ–‡ä»¶ï¼šsrc/graph/nodes.py ç¬¬ 870-908 è¡Œ
async def simple_researcher_node(state: State, config: RunnableConfig) -> Command[Literal["__end__"]]:
    """é¤é¥®æ™ºèƒ½åŠ©æ‰‹èŠ‚ç‚¹"""
    logger.info("é¤é¥®æ™ºèƒ½åŠ©æ‰‹èŠ‚ç‚¹è¿è¡Œä¸­")
    configurable = Configuration.from_runnable_config(config)
    
    # ... å‰é¢çš„ BM25 æœç´¢ç­‰å‡†å¤‡å·¥ä½œä¸å˜ ...
    
    llm = get_llm_by_type(AGENT_LLM_MAP["reporter"])
    logger.info(f"å¼€å§‹ç”Ÿæˆä¸“ä¸šè§£ç­”ï¼ŒLLM: {AGENT_LLM_MAP['reporter']}")
    
    response_content = ""
    try:
        # âœ… æ”¹ç”¨ invokeï¼Œä¸æ¶ˆè´¹æµ
        response = llm.invoke(invoke_messages)
        response_content = response.content if hasattr(response, 'content') else str(response)
        
        if not response_content:
            logger.warning("æµå¼å†…å®¹ä¸ºç©º")
            response_content = "æŠ±æ­‰ï¼Œæœ¬æ¬¡å›ç­”ä¸ºç©ºã€‚"
            
    except Exception as e:
        logger.error(f"LLM è°ƒç”¨å¼‚å¸¸: {e}", exc_info=True)
        response_content = f"æŠ±æ­‰ï¼Œç”Ÿæˆè§£ç­”æ—¶å‡ºç°é”™è¯¯: {e}"
    
    logger.info(f"simple_researcher å“åº”é•¿åº¦: {len(response_content)}")
    
    return Command(
        update={
            "final_report": response_content,
        }
    )
```

---

## ğŸ§ª æµ‹è¯•éªŒè¯

### 1. åŸºçº¿æµ‹è¯•ï¼ˆåº”è¯¥ä»ç„¶é€šè¿‡ï¼‰

```bash
python3 test_openai_stream_baseline.py
```

### 2. API æµå¼æµ‹è¯•

```bash
curl -X POST http://localhost:8000/api/chat/stream \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [{"role": "user", "content": "count to 10 slowly"}],
    "enable_simple_research": true
  }' \
  --no-buffer
```

è§‚å¯Ÿè¾“å‡ºæ˜¯å¦é€ token è¿”å›ï¼ˆåº”è¯¥çœ‹åˆ° `data: {...}` ä¸€æ¡æ¡å‡ºç°ï¼‰ã€‚

### 3. å‰ç«¯æµ‹è¯•

åœ¨ Web UI ä¸­æé—®ï¼Œè§‚å¯Ÿæ˜¯å¦æœ‰"æ‰“å­—æœº"æ•ˆæœã€‚

---

## ğŸ“Š é¢„æœŸæ•ˆæœ

ä¿®å¤åï¼š
- âœ… é¦– token å»¶è¿Ÿï¼ˆTTFBï¼‰< 1 ç§’
- âœ… é€ä¸ª token æ˜¾ç¤ºå†…å®¹
- âœ… å‰ç«¯çœ‹åˆ°æµç•…çš„"æ‰“å­—æœº"æ•ˆæœ
- âœ… ç”¨æˆ·ä½“éªŒæå‡

---

## ğŸ“ ç›¸å…³æ–‡ä»¶

- âœ… **åŸºçº¿æµ‹è¯•è„šæœ¬**ï¼š`test_openai_stream_baseline.py`
- ğŸ“‹ **è¯Šæ–­æ–‡æ¡£**ï¼š`diagnose_streaming_issue.md`
- ğŸ“‹ **ä¿®å¤æŒ‡å—**ï¼š`STREAMING_FIX_GUIDE.md`
- ğŸ“‹ **å¯¹æ¯”æµ‹è¯•**ï¼š`test_streaming_fix_comparison.md`
- ğŸ“‹ **æœ¬æŠ¥å‘Š**ï¼š`STREAMING_DIAGNOSIS_FINAL.md`

---

## ğŸ¯ ä¸‹ä¸€æ­¥è¡ŒåŠ¨

1. **ç«‹å³å®æ–½ä¿®å¤**ï¼šæŒ‰ç…§ä¸Šè¿°æ­¥éª¤ä¿®æ”¹ 3 ä¸ªæ–‡ä»¶
2. **æµ‹è¯•éªŒè¯**ï¼šè¿è¡ŒåŸºçº¿æµ‹è¯•å’Œ API æµ‹è¯•
3. **å‰ç«¯éªŒè¯**ï¼šåœ¨ Web UI ä¸­æµ‹è¯•ç”¨æˆ·ä½“éªŒ

éœ€è¦æˆ‘å¸®ä½ å®æ–½ä¿®å¤å—ï¼Ÿ


