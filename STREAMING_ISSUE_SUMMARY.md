# LangGraph æµå¼è¾“å‡ºé—®é¢˜æ’æŸ¥æ€»ç»“

## âœ… åŸºçº¿æµ‹è¯•ç»“æœ

å·²åˆ›å»ºå¹¶è¿è¡Œ `test_openai_stream_baseline.py`ï¼Œæµ‹è¯•ç»“æœï¼š

- âœ… **OpenAI API æœ¬èº«æ”¯æŒæµå¼è¾“å‡º**
- é¦– token å»¶è¿Ÿ (TTFB): 0.01s
- æµå¼ä¼ è¾“æ­£å¸¸ï¼Œé€ token æ¨é€

**ç»“è®º**ï¼šæ¨¡å‹ç«¯å’Œ SDK æ­£å¸¸ï¼Œé—®é¢˜åœ¨ LangGraph/æœåŠ¡ç«¯å®ç°ã€‚

## ğŸ” é—®é¢˜å®šä½

### é—®é¢˜ä»£ç ä½ç½®

åœ¨ `src/graph/nodes.py` ä¸­çš„ä»¥ä¸‹èŠ‚ç‚¹å­˜åœ¨é—®é¢˜ï¼š

1. **`reporter_node`** (ç¬¬ 336-359 è¡Œ)
2. **`simple_researcher_node`** (ç¬¬ 870-908 è¡Œ)

### é—®é¢˜ä»£ç æ¨¡å¼

```python
# âŒ å½“å‰å®ç°ï¼ˆé”™è¯¯ï¼‰
response_content = ""
for chunk in llm.stream(invoke_messages):  # LLM æœ¬èº«æ˜¯æµå¼çš„
    if getattr(chunk, "content", None):
        response_content += chunk.content  # å…ˆæ”¶é›†æ‰€æœ‰å†…å®¹

return {
    "final_report": response_content,
    "messages": [AIMessage(content=response_content, name="reporter")],  # ä¸€æ¬¡æ€§è¿”å›å®Œæ•´æ¶ˆæ¯
}
```

**é—®é¢˜**ï¼š
- èŠ‚ç‚¹å†…éƒ¨å…ˆæ”¶é›†æ‰€æœ‰ chunk
- ç„¶åä¸€æ¬¡æ€§è¿”å›å®Œæ•´çš„ `AIMessage`
- LangGraph çš„ `stream_mode="messages"` åªèƒ½æ•è·åˆ°å®Œæ•´çš„ `AIMessage`ï¼Œæ— æ³•æµå¼ä¼ é€’

## ğŸ’¡ è§£å†³æ–¹æ¡ˆ

### æ–¹æ¡ˆ 1ï¼šä½¿ç”¨ Runnable åŒ…è£… LLMï¼ˆæ¨èï¼‰

å°† LLM çš„æµå¼è°ƒç”¨ç›´æ¥ä½œä¸ºèŠ‚ç‚¹ï¼Œè®© LangGraph è‡ªåŠ¨å¤„ç†æµå¼è¾“å‡ºï¼š

```python
from langchain_core.runnables import RunnableLambda
from langchain_core.messages import AIMessageChunk

def reporter_node(state: State, config: RunnableConfig):
    # ... å‰é¢çš„ä»£ç ä¸å˜ ...
    
    llm = get_llm_by_type(AGENT_LLM_MAP["reporter"])
    
    # åˆ›å»ºä¸€ä¸ª Runnableï¼Œè®© LangGraph è‡ªåŠ¨æµå¼ä¼ é€’
    def stream_llm_wrapper(inputs):
        """åŒ…è£… LLM streamï¼Œå®æ—¶ yield æ¯ä¸ª chunk"""
        full_content = ""
        for chunk in llm.stream(invoke_messages):
            if hasattr(chunk, "content") and chunk.content:
                full_content += chunk.content
                # å®æ—¶ yield chunkï¼Œè®© LangGraph æ•è·
                yield chunk
        
        # è¿”å›å®Œæ•´å†…å®¹ç”¨äºçŠ¶æ€æ›´æ–°ï¼ˆå¯é€‰ï¼‰
        return full_content
    
    # ä½¿ç”¨ Runnable ä½œä¸ºèŠ‚ç‚¹
    stream_runnable = RunnableLambda(stream_llm_wrapper)
    
    # æ‰§è¡Œå¹¶æ”¶é›†ç»“æœ
    full_content = ""
    for chunk in stream_runnable.stream({}):
        full_content += chunk if isinstance(chunk, str) else getattr(chunk, "content", "")
    
    return {
        "final_report": full_content,
        "messages": [AIMessage(content=full_content, name="reporter")],
    }
```

**æ³¨æ„**ï¼šè¿™ä¸ªæ–¹æ¡ˆä»éœ€è¦èŠ‚ç‚¹æ”¶é›†å†…å®¹ç”¨äºçŠ¶æ€æ›´æ–°ï¼Œä½†å¯ä»¥è®© LangGraph æ•è·åˆ°æµå¼çš„ chunkã€‚

### æ–¹æ¡ˆ 2ï¼šåœ¨æœåŠ¡ç«¯æ‹†åˆ†å®Œæ•´æ¶ˆæ¯ï¼ˆä¸´æ—¶æ–¹æ¡ˆï¼‰

åœ¨ `src/server/app.py` çš„ `_process_message_chunk` å‡½æ•°ä¸­ï¼Œæ£€æµ‹åˆ°å®Œæ•´ `AIMessage` æ—¶ï¼Œå°è¯•æ‹†åˆ†å¹¶æµå¼ä¼ é€’ï¼š

```python
async def _process_message_chunk(message_chunk, message_metadata, thread_id, agent):
    """Process a single message chunk and yield appropriate events."""
    # ... ç°æœ‰ä»£ç  ...
    
    elif isinstance(message_chunk, AIMessage):
        # å¦‚æœæ”¶åˆ°å®Œæ•´çš„ AIMessageï¼Œå°è¯•æ‹†åˆ†å¹¶æµå¼ä¼ é€’
        content = message_chunk.content if hasattr(message_chunk, 'content') else ""
        if content and len(content) > 10:  # åªåœ¨å†…å®¹è¾ƒé•¿æ—¶æ‹†åˆ†
            # æŒ‰å­—ç¬¦æˆ–è¯æ‹†åˆ†ï¼Œæ¨¡æ‹Ÿæµå¼è¾“å‡º
            chunk_size = 5  # æ¯æ¬¡ä¼ é€’çš„å­—ç¬¦æ•°
            for i in range(0, len(content), chunk_size):
                chunk_content = content[i:i+chunk_size]
                chunk_msg = AIMessageChunk(
                    content=chunk_content,
                    name=message_chunk.name if hasattr(message_chunk, 'name') else None
                )
                event_stream_message = _create_event_stream_message(
                    chunk_msg, message_metadata, thread_id, agent_name
                )
                yield _make_event("message_chunk", event_stream_message)
                await asyncio.sleep(0.01)  # æ¨¡æ‹Ÿæµå¼å»¶è¿Ÿ
            
            # æœ€åå‘é€å®Œæˆäº‹ä»¶
            event_stream_message = _create_event_stream_message(
                message_chunk, message_metadata, thread_id, agent_name
            )
            event_stream_message["finish_reason"] = "stop"
            yield _make_event("message_chunk", event_stream_message)
        else:
            # å†…å®¹è¾ƒçŸ­ï¼Œç›´æ¥ä¼ é€’
            event_stream_message["finish_reason"] = "stop"
            yield _make_event("message_chunk", event_stream_message)
```

### æ–¹æ¡ˆ 3ï¼šä¿®æ”¹èŠ‚ç‚¹é€»è¾‘ï¼Œå®æ—¶æ›´æ–°çŠ¶æ€ï¼ˆå¤æ‚ä½†æœ€æ­£ç¡®ï¼‰

ä¿®æ”¹èŠ‚ç‚¹ï¼Œè®©æ¯ä¸ª chunk éƒ½å®æ—¶æ›´æ–°åˆ°çŠ¶æ€ä¸­ã€‚ä½†è¿™éœ€è¦ä¿®æ”¹ LangGraph çš„æ‰§è¡Œæœºåˆ¶ï¼Œæ¯”è¾ƒå¤æ‚ã€‚

## ğŸ“ ä¸‹ä¸€æ­¥è¡ŒåŠ¨

1. **ç«‹å³è¡ŒåŠ¨**ï¼šä¿®æ”¹ `reporter_node` å’Œ `simple_researcher_node`ï¼Œä½¿ç”¨æ–¹æ¡ˆ 1 æˆ–æ–¹æ¡ˆ 2
2. **æµ‹è¯•éªŒè¯**ï¼šè¿è¡Œæµ‹è¯•ç¡®ä¿æµå¼è¾“å‡ºæ­£å¸¸å·¥ä½œ
3. **æ€§èƒ½ä¼˜åŒ–**ï¼šæ ¹æ®éœ€è¦è°ƒæ•´æµå¼è¾“å‡ºçš„ç²’åº¦

## ğŸ§ª æµ‹è¯•å·¥å…·

å·²åˆ›å»ºçš„æµ‹è¯•è„šæœ¬ï¼š

1. **`test_openai_stream_baseline.py`** - åŸºçº¿æµ‹è¯•ï¼ŒéªŒè¯ OpenAI API æµå¼è¾“å‡º
2. **`test_langgraph_stream.py`** - æµ‹è¯• LangGraph ä¸­çš„æµå¼è¾“å‡ºï¼ˆéœ€è¦å®‰è£…ä¾èµ–åè¿è¡Œï¼‰

## å‚è€ƒèµ„æ–™

- [LangGraph æµå¼è¾“å‡ºæ–‡æ¡£](https://github.langchain.ac.cn/langgraph/how-tos/streaming/)
- [LangChain Runnable æ–‡æ¡£](https://python.langchain.com/docs/expression_language/)


