#!/usr/bin/env python3
"""
æµ‹è¯• LangGraph ä¸­çš„æµå¼è¾“å‡º

å¯¹æ¯”ï¼š
1. ç›´æ¥ä½¿ç”¨ llm.stream() çš„è¾“å‡º
2. é€šè¿‡ LangGraph èŠ‚ç‚¹åçš„è¾“å‡º
"""

import asyncio
import sys
import time
from pathlib import Path

project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from langchain_core.messages import HumanMessage, AIMessageChunk
from langgraph.graph import StateGraph, END
from langgraph.types import State
from src.llms.llm import get_llm_by_type

# æµ‹è¯•çŠ¶æ€ç±»å‹
class TestState(State):
    messages: list = []

def test_direct_llm_stream():
    """æµ‹è¯•ç›´æ¥ä½¿ç”¨ llm.stream()"""
    print("=" * 60)
    print("æµ‹è¯• 1ï¼šç›´æ¥ä½¿ç”¨ llm.stream()")
    print("=" * 60)
    
    llm = get_llm_by_type("basic")
    messages = [HumanMessage(content="count to 10 slowly")]
    
    print("å¼€å§‹æµå¼è¾“å‡º...")
    start_time = time.time()
    chunk_count = 0
    
    for chunk in llm.stream(messages):
        if hasattr(chunk, 'content') and chunk.content:
            print(chunk.content, end="", flush=True)
            chunk_count += 1
            time.sleep(0.01)  # æ¨¡æ‹Ÿè§‚å¯Ÿå»¶è¿Ÿ
    
    print()
    print(f"\nå®Œæˆ: {chunk_count} chunks, è€—æ—¶ {time.time() - start_time:.2f}s")
    print()


def test_node_with_stream_collection():
    """æµ‹è¯•èŠ‚ç‚¹å†…éƒ¨æ”¶é›†æ‰€æœ‰ chunk ç„¶åè¿”å›ï¼ˆå½“å‰å®ç°ï¼‰"""
    print("=" * 60)
    print("æµ‹è¯• 2ï¼šèŠ‚ç‚¹å†…éƒ¨æ”¶é›†æ‰€æœ‰ chunkï¼ˆå½“å‰å®ç°æ–¹å¼ï¼‰")
    print("=" * 60)
    
    def stream_collection_node(state: TestState):
        """æ¨¡æ‹Ÿå½“å‰ reporter_node çš„å®ç°"""
        llm = get_llm_by_type("basic")
        messages = [HumanMessage(content="count to 10 slowly")]
        
        print("èŠ‚ç‚¹å¼€å§‹æ‰§è¡Œ...")
        start_time = time.time()
        
        response_content = ""
        chunk_count = 0
        
        for chunk in llm.stream(messages):
            if hasattr(chunk, 'content') and chunk.content:
                response_content += chunk.content  # âŒ å…ˆæ”¶é›†
                chunk_count += 1
        
        print(f"èŠ‚ç‚¹å®Œæˆ: æ”¶é›†äº† {chunk_count} chunksï¼Œè€—æ—¶ {time.time() - start_time:.2f}s")
        print("èŠ‚ç‚¹è¿”å›å®Œæ•´æ¶ˆæ¯...")
        
        return {
            "messages": [AIMessageChunk(content=response_content)]
        }
    
    # æ„å»ºå›¾
    graph = StateGraph(TestState)
    graph.add_node("stream_collection", stream_collection_node)
    graph.set_entry_point("stream_collection")
    graph.add_edge("stream_collection", END)
    
    app = graph.compile()
    
    print("å¼€å§‹æµå¼æ‰§è¡Œå›¾...")
    start_time = time.time()
    
    async def run_test():
        chunk_count = 0
        async for chunk in app.astream(
            {"messages": []},
            stream_mode=["messages"]
        ):
            # æ£€æŸ¥æ˜¯å¦æ˜¯æ¶ˆæ¯ chunk
            for node_name, messages in chunk.items():
                for msg in messages:
                    if isinstance(msg, AIMessageChunk) and hasattr(msg, 'content'):
                        print(msg.content, end="", flush=True)
                        chunk_count += 1
        
        print()
        print(f"\nå®Œæˆ: æ”¶åˆ° {chunk_count} ä¸ªæ¶ˆæ¯äº‹ä»¶ï¼Œè€—æ—¶ {time.time() - start_time:.2f}s")
    
    asyncio.run(run_test())
    print()


def test_node_with_chunk_passthrough():
    """æµ‹è¯•èŠ‚ç‚¹ç›´æ¥ä¼ é€’æ¯ä¸ª chunkï¼ˆç†æƒ³å®ç°ï¼‰"""
    print("=" * 60)
    print("æµ‹è¯• 3ï¼šèŠ‚ç‚¹ç›´æ¥ä¼ é€’æ¯ä¸ª chunkï¼ˆç†æƒ³å®ç°ï¼‰")
    print("=" * 60)
    print("âš ï¸  æ³¨æ„ï¼šè¿™ä¸ªæµ‹è¯•å±•ç¤ºç†æƒ³æƒ…å†µï¼Œä½† LangGraph èŠ‚ç‚¹å‡½æ•°ä¸èƒ½ yield")
    print()
    
    def chunk_passthrough_node(state: TestState):
        """ç†æƒ³æƒ…å†µï¼šæ¯ä¸ª chunk éƒ½å®æ—¶ä¼ é€’"""
        llm = get_llm_by_type("basic")
        messages = [HumanMessage(content="count to 10 slowly")]
        
        print("èŠ‚ç‚¹å¼€å§‹æ‰§è¡Œï¼ˆç†æƒ³æƒ…å†µï¼šåº”è¯¥æ¯ä¸ª chunk éƒ½å®æ—¶ä¼ é€’ï¼‰...")
        print("âš ï¸  ä½†èŠ‚ç‚¹å‡½æ•°åªèƒ½è¿”å›ä¸€æ¬¡ï¼Œæ‰€ä»¥è¿™ä¸ªæµ‹è¯•ä¼šå¤±è´¥")
        print()
        
        # å®é™…å®ç°ä¸­ï¼Œæˆ‘ä»¬éœ€è¦å…¶ä»–æ–¹å¼æ¥ä¼ é€’ chunk
        response_content = ""
        for chunk in llm.stream(messages):
            if hasattr(chunk, 'content') and chunk.content:
                response_content += chunk.content
        
        return {
            "messages": [AIMessageChunk(content=response_content)]
        }
    
    # æ„å»ºå›¾
    graph = StateGraph(TestState)
    graph.add_node("chunk_passthrough", chunk_passthrough_node)
    graph.set_entry_point("chunk_passthrough")
    graph.add_edge("chunk_passthrough", END)
    
    app = graph.compile()
    
    print("å¼€å§‹æµå¼æ‰§è¡Œå›¾...")
    start_time = time.time()
    
    async def run_test():
        chunk_count = 0
        async for chunk in app.astream(
            {"messages": []},
            stream_mode=["messages"]
        ):
            for node_name, messages in chunk.items():
                for msg in messages:
                    if isinstance(msg, AIMessageChunk) and hasattr(msg, 'content'):
                        print(msg.content, end="", flush=True)
                        chunk_count += 1
        
        print()
        print(f"\nå®Œæˆ: æ”¶åˆ° {chunk_count} ä¸ªæ¶ˆæ¯äº‹ä»¶ï¼Œè€—æ—¶ {time.time() - start_time:.2f}s")
    
    asyncio.run(run_test())
    print()


if __name__ == "__main__":
    print("\nğŸ” LangGraph æµå¼è¾“å‡ºè¯Šæ–­æµ‹è¯•\n")
    
    # æµ‹è¯•1ï¼šç›´æ¥ LLM æµå¼
    test_direct_llm_stream()
    
    # æµ‹è¯•2ï¼šèŠ‚ç‚¹æ”¶é›†åè¿”å›
    test_node_with_stream_collection()
    
    # æµ‹è¯•3ï¼šç†æƒ³æƒ…å†µ
    test_node_with_chunk_passthrough()
    
    print("=" * 60)
    print("è¯Šæ–­æ€»ç»“ï¼š")
    print("1. å¦‚æœæµ‹è¯•1æ˜¯æµå¼çš„ï¼Œä½†æµ‹è¯•2ä¸æ˜¯ï¼Œè¯´æ˜é—®é¢˜åœ¨èŠ‚ç‚¹å®ç°")
    print("2. LangGraph èŠ‚ç‚¹å‡½æ•°ä¸èƒ½ yieldï¼Œæ‰€ä»¥éœ€è¦ç‰¹æ®Šå¤„ç†æ‰èƒ½å®ç°çœŸæ­£çš„æµå¼")
    print("=" * 60)


