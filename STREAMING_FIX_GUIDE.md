# LangGraph æµå¼è¾“å‡ºé—®é¢˜ä¿®å¤æŒ‡å—

## ğŸ” é—®é¢˜ç¡®è®¤

### âœ… å·²éªŒè¯æ­£å¸¸çš„éƒ¨åˆ†

1. **OpenAI API å±‚**ï¼š`test_openai_stream_baseline.py` æµ‹è¯•é€šè¿‡ï¼Œæ¨¡å‹ç«¯æ”¯æŒæµå¼è¾“å‡º
2. **æœåŠ¡ç«¯ä»£ç **ï¼š`src/server/app.py` ä¸­çš„ `_astream_workflow_generator` æ­£ç¡®ä½¿ç”¨ `yield event`ï¼Œæ²¡æœ‰å…ˆæ”¶é›†

```python
# âœ… æœåŠ¡ç«¯ä»£ç æ­£ç¡®
async for event in _stream_graph_events(...):
    yield event  # é€ä¸ª yieldï¼Œä¸æ”¶é›†
```

3. **LangGraph é…ç½®**ï¼šä½¿ç”¨äº†æ­£ç¡®çš„ `stream_mode=["messages", "updates"]`

```python
# âœ… LangGraph é…ç½®æ­£ç¡®
async for agent, _, event_data in graph_instance.astream(
    workflow_input,
    config=workflow_config,
    stream_mode=["messages", "updates"],  # âœ… æ­£ç¡®
    subgraphs=True,
):
```

### âŒ é—®é¢˜æ‰€åœ¨ï¼šèŠ‚ç‚¹å®ç°

åœ¨ `src/graph/nodes.py` ä¸­ï¼š

```python
# âŒ é—®é¢˜ä»£ç ï¼ˆç¬¬ 336-359 è¡Œï¼Œreporter_nodeï¼‰
def reporter_node(state: State, config: RunnableConfig):
    # ...
    llm = get_llm_by_type(AGENT_LLM_MAP["reporter"])
    
    response_content = ""
    for chunk in llm.stream(invoke_messages):  # âŒ èŠ‚ç‚¹å†…éƒ¨æ¶ˆè´¹äº†æµ
        if getattr(chunk, "content", None):
            response_content += chunk.content
    
    return {
        "final_report": response_content,
        "messages": [AIMessage(content=response_content, name="reporter")],  # âŒ ä¸€æ¬¡æ€§è¿”å›
    }
```

**æ ¸å¿ƒé—®é¢˜**ï¼š
- èŠ‚ç‚¹å†…éƒ¨çš„ `for chunk in llm.stream()` **æ¶ˆè´¹æ‰äº†æµ**
- LangGraph çš„ `stream_mode="messages"` æ— æ³•æ•è·å·²è¢«æ¶ˆè´¹çš„ chunk
- èŠ‚ç‚¹æœ€åä¸€æ¬¡æ€§è¿”å›å®Œæ•´çš„ `AIMessage`ï¼Œæ‰€ä»¥å‰ç«¯çœ‹åˆ°çš„æ˜¯ä¸€æ¬¡æ€§è¿”å›

## ğŸ’¡ è§£å†³æ–¹æ¡ˆ

### æ–¹æ¡ˆ 1ï¼šä½¿ç”¨ LangChain çš„ Runnable è®©èŠ‚ç‚¹æ”¯æŒæµå¼ï¼ˆæ¨èï¼‰

å…³é”®ï¼šä¸åœ¨èŠ‚ç‚¹å‡½æ•°ä¸­æ‰‹åŠ¨è°ƒç”¨ `llm.stream()`ï¼Œè€Œæ˜¯è®© LLM ä½œä¸º Runnable ç›´æ¥ç»‘å®šåˆ°èŠ‚ç‚¹ã€‚

ä½† LangGraph çš„èŠ‚ç‚¹å¿…é¡»æ˜¯å‡½æ•°ï¼Œä¸èƒ½ç›´æ¥ç”¨ Runnableã€‚è§£å†³æ–¹æ³•æ˜¯åœ¨èŠ‚ç‚¹ä¸­**ä¸æ¶ˆè´¹æµ**ï¼š

```python
def reporter_node(state: State, config: RunnableConfig):
    """ä¿®æ”¹åçš„ reporter_node"""
    # ... å‰é¢çš„å‡†å¤‡å·¥ä½œä¸å˜ ...
    
    llm = get_llm_by_type(AGENT_LLM_MAP["reporter"])
    
    # âœ… æ–¹æ¡ˆï¼šç›´æ¥ invokeï¼Œè®© LangGraph åœ¨åº•å±‚å¤„ç†æµå¼
    # LangGraph ä¼šè‡ªåŠ¨æ•è· LLM çš„æµå¼è¾“å‡º
    response = llm.invoke(invoke_messages)
    
    return {
        "final_report": response.content,
        "messages": [response],  # è¿”å›å®Œæ•´æ¶ˆæ¯ç”¨äºçŠ¶æ€æ›´æ–°
    }
```

**æ³¨æ„**ï¼šè™½ç„¶è¿™é‡Œç”¨çš„æ˜¯ `invoke`ï¼Œä½†å¦‚æœ LLM åœ¨å†…éƒ¨é…ç½®äº† `streaming=True`ï¼ŒLangGraph ä»ç„¶å¯ä»¥æ•è·æµå¼è¾“å‡ºã€‚

### æ–¹æ¡ˆ 2ï¼šè®© LLM é…ç½®æµå¼å›è°ƒï¼ˆéœ€è¦éªŒè¯ï¼‰

åœ¨åˆ›å»º LLM æ—¶é…ç½®æµå¼å›è°ƒï¼š

```python
# åœ¨ src/llms/llm.py ä¸­
llm = ChatOpenAI(
    model=model,
    streaming=True,  # âœ… å¯ç”¨æµå¼
    callbacks=[...]  # é…ç½®å›è°ƒ
)
```

ä½†è¿™éœ€è¦ç¡®è®¤ LangGraph èƒ½å¦æ•è·è¿™äº›å›è°ƒã€‚

### æ–¹æ¡ˆ 3ï¼šä¿®æ”¹èŠ‚ç‚¹ä½¿ç”¨å¼‚æ­¥ç”Ÿæˆå™¨ï¼ˆå¤æ‚ï¼‰

LangGraph æ”¯æŒå¼‚æ­¥ç”Ÿæˆå™¨èŠ‚ç‚¹ï¼Œä½†éœ€è¦ä¿®æ”¹èŠ‚ç‚¹ç­¾åå’Œå›¾çš„æ„å»ºæ–¹å¼ã€‚

## ğŸ¯ æ¨èä¿®å¤æ­¥éª¤

### æ­¥éª¤ 1ï¼šä¿®æ”¹ `simple_researcher_node`

```python
# åœ¨ src/graph/nodes.py ç¬¬ 870-908 è¡Œ
async def simple_researcher_node(state: State, config: RunnableConfig) -> Command[Literal["__end__"]]:
    """é¤é¥®æ™ºèƒ½åŠ©æ‰‹èŠ‚ç‚¹ï¼ˆä¿®å¤æµå¼è¾“å‡ºï¼‰"""
    logger.info("é¤é¥®æ™ºèƒ½åŠ©æ‰‹èŠ‚ç‚¹è¿è¡Œä¸­")
    configurable = Configuration.from_runnable_config(config)
    
    # ... å‰é¢çš„ BM25 æœç´¢ç­‰å‡†å¤‡å·¥ä½œä¸å˜ ...
    
    llm = get_llm_by_type(AGENT_LLM_MAP["reporter"])
    logger.info(f"å¼€å§‹æµå¼ç”Ÿæˆä¸“ä¸šè§£ç­”ï¼ŒLLM: {AGENT_LLM_MAP['reporter']}")
    
    # âœ… ä¿®å¤ï¼šç›´æ¥ invokeï¼Œè®© LangGraph å¤„ç†æµå¼
    response = llm.invoke(invoke_messages)
    response_content = response.content if hasattr(response, 'content') else str(response)
    
    logger.info(f"simple_researcher å“åº”é•¿åº¦: {len(response_content)}")
    
    return Command(
        update={
            "final_report": response_content,
        }
    )
```

### æ­¥éª¤ 2ï¼šä¿®æ”¹ `reporter_node`

```python
# åœ¨ src/graph/nodes.py ç¬¬ 336-359 è¡Œ
def reporter_node(state: State, config: RunnableConfig):
    """Reporter node that write a final report."""
    logger.info("Reporter write final report")
    configurable = Configuration.from_runnable_config(config)
    
    # ... å‰é¢çš„å‡†å¤‡å·¥ä½œä¸å˜ ...
    
    llm = get_llm_by_type(AGENT_LLM_MAP["reporter"]) 
    
    # âœ… ä¿®å¤ï¼šç›´æ¥ invokeï¼Œè®© LangGraph å¤„ç†æµå¼
    try:
        logger.info("Reporterå¼€å§‹ç”ŸæˆæŠ¥å‘Š...")
        response = llm.invoke(invoke_messages)
        response_content = response.content if hasattr(response, 'content') else str(response)
        
        if not response_content:
            logger.warning("Reporterå†…å®¹ä¸ºç©º")
            response_content = "æŠ±æ­‰ï¼Œç”ŸæˆæŠ¥å‘Šå¤±è´¥ã€‚"
            
    except Exception as e:
        logger.error(f"Reporter LLMè°ƒç”¨å¼‚å¸¸: {e}", exc_info=True)
        response_content = f"æŠ±æ­‰ï¼Œç”ŸæˆæŠ¥å‘Šæ—¶å‡ºç°é”™è¯¯: {str(e)}"
    
    logger.info(f"reporter response length: {len(response_content)}")
    
    return {
        "final_report": response_content,
        "messages": [AIMessage(content=response_content, name="reporter")],
    }
```

### æ­¥éª¤ 3ï¼šç¡®è®¤ LLM é…ç½®äº†æµå¼

æ£€æŸ¥ `src/llms/llm.py`ï¼Œç¡®ä¿ LLM åˆ›å»ºæ—¶å¯ç”¨äº†æµå¼ï¼š

```python
# åœ¨åˆ›å»º ChatOpenAI æ—¶æ·»åŠ 
return ChatOpenAI(
    **merged_conf,
    streaming=True,  # âœ… ç¡®ä¿å¯ç”¨æµå¼
)
```

## ğŸ§ª æµ‹è¯•éªŒè¯

ä¿®æ”¹åï¼Œè¿è¡Œä»¥ä¸‹æµ‹è¯•ï¼š

1. **åŸºçº¿æµ‹è¯•**ï¼š`python3 test_openai_stream_baseline.py`ï¼ˆåº”è¯¥ä»ç„¶é€šè¿‡ï¼‰
2. **API æµ‹è¯•**ï¼šè°ƒç”¨ `/api/chat/stream` ç«¯ç‚¹ï¼Œè§‚å¯Ÿæ˜¯å¦é€ token è¿”å›
3. **å‰ç«¯æµ‹è¯•**ï¼šåœ¨ Web UI ä¸­è§‚å¯Ÿæ˜¯å¦é€å­—æ˜¾ç¤º

## ğŸ“Š é¢„æœŸæ•ˆæœ

ä¿®æ”¹åï¼Œæµå¼è¾“å‡ºåº”è¯¥ï¼š
- é¦– token å»¶è¿Ÿï¼ˆTTFBï¼‰< 1 ç§’
- é€ä¸ª token æ˜¾ç¤ºï¼Œè€Œä¸æ˜¯ä¸€æ¬¡æ€§æ˜¾ç¤ºå…¨éƒ¨å†…å®¹
- ç”¨æˆ·ä½“éªŒæµç•…ï¼Œçœ‹åˆ°"æ‰“å­—æœº"æ•ˆæœ

## âš ï¸  æ³¨æ„äº‹é¡¹

1. **ä¸è¦åœ¨èŠ‚ç‚¹ä¸­ä½¿ç”¨ `llm.stream()`**ï¼šè¿™ä¼šæ¶ˆè´¹æ‰æµï¼ŒLangGraph æ— æ³•æ•è·
2. **ä½¿ç”¨ `llm.invoke()` é…åˆ `streaming=True`**ï¼šè®© LangGraph åœ¨åº•å±‚å¤„ç†æµå¼
3. **æµ‹è¯•éªŒè¯**ï¼šä¿®æ”¹ååŠ¡å¿…æµ‹è¯•ç¡®è®¤æµå¼è¾“å‡ºæ­£å¸¸

## ğŸ”— å‚è€ƒèµ„æ–™

- [LangGraph æµå¼è¾“å‡ºæ–‡æ¡£](https://github.langchain.ac.cn/langgraph/how-tos/streaming/)
- [LangChain æµå¼è¾“å‡º](https://python.langchain.com/docs/how_to/streaming/)


